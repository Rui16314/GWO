def GWOM_modified_phased_exploitation_first(objf, lb, ub, dim, SearchAgents_no, Max_iter,
                                            cycles_per_phase=5,
                                            initial_a=2.0,
                                            no_repeat=True):
    s = solution()
    s.cycles = cycles_per_phase

    if not isinstance(lb, list):
        lb = [lb] * dim
    if not isinstance(ub, list):
        ub = [ub] * dim

    Positions = numpy.zeros((SearchAgents_no, dim))
    for i in range(dim):
        Positions[:, i] = numpy.random.uniform(lb[i], ub[i], SearchAgents_no)

    Alpha_pos = numpy.zeros(dim)
    Alpha_score = float("inf")
    Beta_pos = numpy.zeros(dim)
    Beta_score = float("inf")
    Delta_pos = numpy.zeros(dim)
    Delta_score = float("inf")

    # Store bests. 'original_bests' will now be updated at the beginning of each EXPLOITATION phase,
    # as these are the solutions we want to reintroduce to encourage convergence later.
    original_bests = {
        'Alpha': {'pos': None, 'score': float('inf')},
        'Beta': {'pos': None, 'score': float('inf')},
        'Delta': {'pos': None, 'score': float('inf')}
    }

    Convergence_curve = numpy.zeros(Max_iter)
    timerStart = time.time()

    # Total length of one full cycle (Exploitation + Exploration)
    total_cycle_length = 2 * cycles_per_phase

    for l in range(Max_iter):
        # Calculate 'a' which linearly decreases from initial_a to 0 over Max_iter
        a = initial_a - l * (initial_a / Max_iter)
        s.a_values.append(a)

        # Determine the phase type based on the iteration within the current cycle
        cycle_iteration = l % total_cycle_length

        phase_type = ''
        r1_mode_is_outer = False # Default for exploitation, which will be the first phase

        # --- Phase 1: Exploitation --- (first cycles_per_phase iterations of a block)
        if cycle_iteration < cycles_per_phase:
            phase_type = 'EXPLOITATION'
            r1_mode_is_outer = False # R1 for exploitation (inner range)
            if cycle_iteration == 0:  # Actions at the very start of a new EXPLOITATION block
                # Save the current best wolves. These will be used for reintroduction later in this exploitation block.
                original_bests['Alpha'] = {'pos': Alpha_pos.copy(), 'score': Alpha_score}
                original_bests['Beta'] = {'pos': Beta_pos.copy(), 'score': Beta_score}
                original_bests['Delta'] = {'pos': Delta_pos.copy(), 'score': Delta_score}

                # Reintroduce bests from the *end of the previous exploration phase* (or initial state)
                # to replace the worst current wolves. This helps focus around known good areas.
                current_fitnesses = numpy.array([objf(pos) for pos in Positions])
                worst_indices = numpy.argsort(current_fitnesses)[-3:] # Get indices of the 3 worst wolves

                for i_idx, wolf_key in enumerate(['Alpha', 'Beta', 'Delta']):
                    if i_idx < len(worst_indices) and original_bests[wolf_key]['pos'] is not None:
                        Positions[worst_indices[i_idx]] = original_bests[wolf_key]['pos'].copy()

        # --- Phase 2: Exploration --- (next cycles_per_phase iterations of a block)
        else: # cycle_iteration >= cycles_per_phase
            phase_type = 'EXPLORATION'
            r1_mode_is_outer = True # R1 for exploration (outer range)
            if cycle_iteration == cycles_per_phase: # Actions at the very start of a new EXPLORATION block
                # Randomize a portion of best wolves at the beginning of exploration to avoid local optima
                current_fitnesses = numpy.array([objf(pos) for pos in Positions])
                best_indices = numpy.argsort(current_fitnesses)[:3]
                for idx in best_indices:
                    Positions[idx] = numpy.random.uniform(lb, ub, dim)


        # Agent update loop
        for i in range(SearchAgents_no):
            Positions[i] = numpy.clip(Positions[i], lb, ub)
            fitness = objf(Positions[i])
            fe = l * SearchAgents_no + i
            s.function_evaluations.append(fe)

            # Update Alpha, Beta, and Delta based on ALL agents' current fitness
            if fitness < Alpha_score:
                Delta_score = Beta_score
                Delta_pos = Beta_pos.copy()
                Beta_score = Alpha_score
                Beta_pos = Alpha_pos.copy()
                Alpha_score = fitness
                Alpha_pos = Positions[i].copy()
            elif fitness < Beta_score:
                Delta_score = Beta_score
                Delta_pos = Beta_pos.copy()
                Beta_score = fitness
                Beta_pos = Positions[i].copy()
            elif fitness < Delta_score:
                Delta_score = fitness
                Delta_pos = Positions[i].copy()

            # Determine r1 value based on the current phase
            if r1_mode_is_outer:
                r1 = random.choice([random.uniform(0, 0.25), random.uniform(0.75, 1)])
            else:
                r1 = random.uniform(0.25, 0.75)

            # Calculate A and C
            A = 2 * a * r1 - a
            C = 2 * random.random()

            # Calculate movement towards Alpha, Beta, Delta
            D_alpha = abs(C * Alpha_pos - Positions[i])
            X1 = Alpha_pos - A * D_alpha
            D_beta = abs(C * Beta_pos - Positions[i])
            X2 = Beta_pos - A * D_beta
            D_delta = abs(C * Delta_pos - Positions[i])
            X3 = Delta_pos - A * D_delta

            # Update agent's position
            new_pos = (X1 + X2 + X3) / 3
            new_pos_clipped = numpy.clip(new_pos, lb, ub)

            # Data Collection (ref_pos is Alpha_pos for classification)
            if no_repeat:
                activity_type, rel_fitness = measure_exploration(Alpha_pos, new_pos_clipped, objf)

                if activity_type in ["SE", "FE"]:
                    s.exp_dict[activity_type].append((fe, rel_fitness))
                else:
                    s.exp_dict[activity_type].append(rel_fitness)

                s.A_values_all_agents.append((fe, A, activity_type))
                s.C_values_all_agents.append((fe, C))

                s.local_optima_history.append(objf(closest_integer_minima(new_pos_clipped.copy())))
                s.alpha_local_optima_history.append(objf(closest_integer_minima(Alpha_pos.copy())))
                s.alpha_fitness_history.append(Alpha_score)

            Positions[i] = new_pos_clipped

        Convergence_curve[l] = Alpha_score
        print(f"Iteration {l + 1}: Alpha Fitness = {Alpha_score}")

    s.endTime = time.strftime("%Y-%m-%d-%H-%M-%S")
    s.executionTime = time.time() - timerStart
    s.convergence = Convergence_curve
    s.optimizer = f"GWOM_2phase_ExploitFirst_a{initial_a}_cpp{s.cycles}" # Name changed to reflect new order
    s.bestIndividual = Alpha_pos
    s.objfname = objf.__name__

    return s

