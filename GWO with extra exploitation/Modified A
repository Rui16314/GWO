def GWOM_modified_phased_exploration_first(objf, lb, ub, dim, SearchAgents_no, Max_iter,
                                           cycles_per_phase=5,
                                           initial_a=2.0): # Removed no_repeat since it's always True now
    s = solution()
    s.cycles = cycles_per_phase

    if not isinstance(lb, list):
        lb = [lb] * dim
    if not isinstance(ub, list):
        ub = [ub] * dim

    Positions = numpy.zeros((SearchAgents_no, dim))
    for i in range(dim):
        Positions[:, i] = numpy.random.uniform(lb[i], ub[i], SearchAgents_no)

    Alpha_pos = numpy.zeros(dim)
    Alpha_score = float("inf")
    Beta_pos = numpy.zeros(dim)
    Beta_score = float("inf")
    Delta_pos = numpy.zeros(dim)
    Delta_score = float("inf")

    original_bests = {
        'Alpha': {'pos': None, 'score': float('inf')},
        'Beta': {'pos': None, 'score': float('inf')},
        'Delta': {'pos': None, 'score': float('inf')}
    }

    Convergence_curve = numpy.zeros(Max_iter)
    timerStart = time.time()

    # The total length of one full cycle (exploration + refined exploitation)
    # We will alternate between these two for the majority of the run.
    total_phase_len = 2 * cycles_per_phase # Exploration + Exploitation phase

    for l in range(Max_iter):
        # Calculate 'a' value based on the current phase
        if l < cycles_per_phase: # Initial Exploration Phase
            # 'a' decreases from initial_a (e.g., 2) to 0 over the first `cycles_per_phase` iterations.
            a = initial_a - l * (initial_a / cycles_per_phase)
            r1_mode_is_outer = True # For exploration, r1 is outer range
            if l == 0: # Trigger at the very beginning of the first exploration phase
                original_bests['Alpha'] = {'pos': Alpha_pos.copy(), 'score': Alpha_score}
                original_bests['Beta'] = {'pos': Beta_pos.copy(), 'score': Beta_score}
                original_bests['Delta'] = {'pos': Delta_pos.copy(), 'score': Delta_score}

                current_fitnesses = numpy.array([objf(pos) for pos in Positions])
                best_indices = numpy.argsort(current_fitnesses)[:3]
                for idx in best_indices:
                    Positions[idx] = numpy.random.uniform(lb, ub, dim) # Randomize best wolves

        else: # Subsequent alternating phases
            # After the initial exploration, 'a' will cycle between 1 and 0
            # such that A is constrained to [0,1].
            # This is done by effectively re-scaling the iteration count for 'a'
            # within the current sub-cycle of length `total_phase_len`.
            iteration_in_alternating_cycle = (l - cycles_per_phase) % total_phase_len

            if iteration_in_alternating_cycle < cycles_per_phase:
                # Exploitation part of the alternating cycle, 'a' goes from 1 to 0
                a = 1 - (iteration_in_alternating_cycle / cycles_per_phase)
                r1_mode_is_outer = False # For exploitation, r1 is inner range
                if iteration_in_alternating_cycle == 0: # Start of a new exploitation block
                    # Revert worst wolves to previous bests from the *previous* exploration block
                    # (which would have been `original_bests` from the last time `l % total_phase_len == 0` for the Exploration phase)
                    current_fitnesses = numpy.array([objf(pos) for pos in Positions])
                    worst_indices = numpy.argsort(current_fitnesses)[-3:]

                    for i_idx, wolf_key in enumerate(['Alpha', 'Beta', 'Delta']):
                        if i_idx < len(worst_indices) and original_bests[wolf_key]['pos'] is not None:
                            Positions[worst_indices[i_idx]] = original_bests[wolf_key]['pos'].copy()

            else:
                # Exploration part of the alternating cycle, 'a' goes from 1 to 0
                a = 1 - ((iteration_in_alternating_cycle - cycles_per_phase) / cycles_per_phase)
                r1_mode_is_outer = True # For exploration, r1 is outer range
                if iteration_in_alternating_cycle == cycles_per_phase: # Start of a new exploration block
                    # Update original_bests from the *end* of the previous exploitation block
                    original_bests['Alpha'] = {'pos': Alpha_pos.copy(), 'score': Alpha_score}
                    original_bests['Beta'] = {'pos': Beta_pos.copy(), 'score': Beta_score}
                    original_bests['Delta'] = {'pos': Delta_pos.copy(), 'score': Delta_score}

                    # Randomize best wolves at the beginning of exploration phase (as per original logic)
                    current_fitnesses = numpy.array([objf(pos) for pos in Positions])
                    best_indices = numpy.argsort(current_fitnesses)[:3]
                    for idx in best_indices:
                        Positions[idx] = numpy.random.uniform(lb, ub, dim)

        s.a_values.append(a) # Store the 'a' value for plotting (although A is what's truly constrained)


        # Agent update loop
        for i in range(SearchAgents_no):
            Positions[i] = numpy.clip(Positions[i], lb, ub)
            fitness = objf(Positions[i])
            fe = l * SearchAgents_no + i
            s.function_evaluations.append(fe)

            if fitness < Alpha_score:
                Delta_score = Beta_score
                Delta_pos = Beta_pos.copy()
                Beta_score = Alpha_score
                Beta_pos = Alpha_pos.copy()
                Alpha_score = fitness
                Alpha_pos = Positions[i].copy()
            elif fitness < Beta_score:
                Delta_score = Beta_score
                Delta_pos = Beta_pos.copy()
                Beta_score = fitness
                Beta_pos = Positions[i].copy()
            elif fitness < Delta_score:
                Delta_score = fitness
                Delta_pos = Positions[i].copy()

            if r1_mode_is_outer:
                r1 = random.choice([random.uniform(0, 0.25), random.uniform(0.75, 1)])
            else:
                r1 = random.uniform(0.25, 0.75)

            A = 2 * a * r1 - a
            C = 2 * random.random()

            D_alpha = abs(C * Alpha_pos - Positions[i])
            X1 = Alpha_pos - A * D_alpha
            D_beta = abs(C * Beta_pos - Positions[i])
            X2 = Beta_pos - A * D_beta
            D_delta = abs(C * Delta_pos - Positions[i])
            X3 = Delta_pos - A * D_delta

            new_pos = (X1 + X2 + X3) / 3
            new_pos_clipped = numpy.clip(new_pos, lb, ub)

            # Data Collection (ref_pos is Alpha_pos)
            activity_type, rel_fitness = measure_exploration(Alpha_pos, new_pos_clipped, objf)

            if activity_type in ["SE", "FE"]:
                s.exp_dict[activity_type].append((fe, rel_fitness))
            else:
                s.exp_dict[activity_type].append(rel_fitness)

            s.A_values_all_agents.append((fe, A, activity_type))
            s.C_values_all_agents.append((fe, C))

            s.local_optima_history.append(objf(closest_integer_minima(new_pos_clipped.copy())))
            s.alpha_local_optima_history.append(objf(closest_integer_minima(Alpha_pos.copy())))
            s.alpha_fitness_history.append(Alpha_score)

            Positions[i] = new_pos_clipped

        Convergence_curve[l] = Alpha_score
        print(f"Iteration {l+1}: Alpha Fitness = {Alpha_score}")

    s.endTime = time.strftime("%Y-%m-%d-%H-%M-%S")
    s.executionTime = time.time() - timerStart
    s.convergence = Convergence_curve
    s.optimizer = f"GWOM_Phased_ExpFirst_A_Constrained_a{initial_a}_cpp{s.cycles}"
    s.bestIndividual = Alpha_pos
    s.objfname = objf.__name__

    return s
