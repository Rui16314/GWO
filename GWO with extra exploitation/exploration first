def GWOM_modified_phased_exploration_first(objf, lb, ub, dim, SearchAgents_no, Max_iter,
                                            cycles_per_phase=5,
                                            initial_a=2.0,
                                            no_repeat=True):
    s = solution()
    s.cycles = cycles_per_phase

    if not isinstance(lb, list):
        lb = [lb] * dim
    if not isinstance(ub, list):
        ub = [ub] * dim

    Positions = numpy.zeros((SearchAgents_no, dim))
    for i in range(dim):
        Positions[:, i] = numpy.random.uniform(lb[i], ub[i], SearchAgents_no)

    Alpha_pos = numpy.zeros(dim)
    Alpha_score = float("inf")
    Beta_pos = numpy.zeros(dim)
    Beta_score = float("inf")
    Delta_pos = numpy.zeros(dim)
    Delta_score = float("inf")

    original_bests = {
        'Alpha': {'pos': None, 'score': float('inf')},
        'Beta': {'pos': None, 'score': float('inf')},
        'Delta': {'pos': None, 'score': float('inf')}
    }

    Convergence_curve = numpy.zeros(Max_iter)
    timerStart = time.time()

    total_metacycle_len = 2 * cycles_per_phase  # As per previous modification: Exploration then Exploitation

    for l in range(Max_iter):
        a = initial_a
        s.a_values.append(a)

        current_iter_in_metacycle = l % total_metacycle_len

        phase_type = ''  # Will be set below

        # --- Phase 1: Exploration --- (first cycles_per_phase iterations)
        if current_iter_in_metacycle < cycles_per_phase:
            phase_type = 'EXPLORATION'
            if current_iter_in_metacycle == 0:  # Trigger at the beginning of the Exploration phase
                original_bests['Alpha'] = {'pos': Alpha_pos.copy(), 'score': Alpha_score}
                original_bests['Beta'] = {'pos': Beta_pos.copy(), 'score': Beta_score}
                original_bests['Delta'] = {'pos': Delta_pos.copy(), 'score': Delta_score}

                current_fitnesses = numpy.array([objf(pos) for pos in Positions])
                best_indices = numpy.argsort(current_fitnesses)[:3]
                for idx in best_indices:
                    Positions[idx] = numpy.random.uniform(lb, ub, dim)  # Randomize best wolves

        # --- Phase 2: Exploitation --- (next cycles_per_phase iterations)
        else:  # current_iter_in_metacycle >= cycles_per_phase
            phase_type = 'EXPLOIT_INITIAL'  # Using this phase type as it implies standard exploitation with inner r1
            # No special actions at the start of this phase (like replacing worst, as per initial exploitation)

        # Agent update loop
        for i in range(SearchAgents_no):
            Positions[i] = numpy.clip(Positions[i], lb, ub)
            fitness = objf(Positions[i])
            fe = l * SearchAgents_no + i
            s.function_evaluations.append(fe)

            if fitness < Alpha_score:
                Delta_score = Beta_score
                Delta_pos = Beta_pos.copy()
                Beta_score = Alpha_score
                Beta_pos = Alpha_pos.copy()
                Alpha_score = fitness
                Alpha_pos = Positions[i].copy()
            elif fitness < Beta_score:
                Delta_score = Beta_score
                Delta_pos = Beta_pos.copy()
                Beta_score = fitness
                Beta_pos = Positions[i].copy()
            elif fitness < Delta_score:
                Delta_score = fitness
                Delta_pos = Positions[i].copy()

            # Determine r1 based on current phase_type
            if phase_type == 'EXPLORATION':
                r1 = random.choice([random.uniform(0, 0.25), random.uniform(0.75, 1)])
            elif phase_type == 'EXPLOIT_INITIAL':  # This is the exploitation phase in the 2-phase cycle
                r1 = random.uniform(0.25, 0.75)
            else:  # Fallback, should not happen
                r1 = random.random()

            A = 2 * a * r1 - a
            C = 2 * random.random()

            D_alpha = abs(C * Alpha_pos - Positions[i])
            X1 = Alpha_pos - A * D_alpha
            D_beta = abs(C * Beta_pos - Positions[i])
            X2 = Beta_pos - A * D_beta
            D_delta = abs(C * Delta_pos - Positions[i])
            X3 = Delta_pos - A * D_delta

            new_pos = (X1 + X2 + X3) / 3
            new_pos_clipped = numpy.clip(new_pos, lb, ub)

            # Data Collection (ref_pos is Alpha_pos)
            if no_repeat:
                activity_type, rel_fitness = measure_exploration(Alpha_pos, new_pos_clipped, objf)

                if activity_type in ["SE", "FE"]:
                    s.exp_dict[activity_type].append((fe, rel_fitness))
                else:
                    s.exp_dict[activity_type].append(rel_fitness)

                s.A_values_all_agents.append((fe, A, activity_type))
                s.C_values_all_agents.append((fe, C))  # Store (fe, C) for consistency

                s.local_optima_history.append(objf(closest_integer_minima(new_pos_clipped.copy())))
                s.alpha_local_optima_history.append(objf(closest_integer_minima(Alpha_pos.copy())))
                s.alpha_fitness_history.append(Alpha_score)

            Positions[i] = new_pos_clipped

        Convergence_curve[l] = Alpha_score
        print(f"Iteration {l + 1}: Alpha Fitness = {Alpha_score}")

    s.endTime = time.strftime("%Y-%m-%d-%H-%M-%S")
    s.executionTime = time.time() - timerStart
    s.convergence = Convergence_curve
    s.optimizer = f"GWOM_2phase_Exp_Exploit_a{initial_a}_cpp{s.cycles}"
    s.bestIndividual = Alpha_pos
    s.objfname = objf.__name__

    return s
